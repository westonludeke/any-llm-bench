"""Report generation for any-llm Bench."""

import json
import os
from typing import Dict, Any


def write_markdown(path: str, context: Dict[str, Any]) -> None:
    """Write a Markdown report to the specified path."""
    
    result1 = context["model1"]
    result2 = context["model2"]
    
    # Create runs directory if it doesn't exist
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    with open(path, 'w') as f:
        f.write(f"# Model Comparison Report\n\n")
        f.write(f"**Timestamp:** {context['timestamp']}\n")
        f.write(f"**Task:** {context['task']}\n")
        f.write(f"**Mock Mode:** {'Yes' if context.get('mock_mode', False) else 'No'}\n\n")
        
        # Prompt snippet (truncated)
        prompt_snippet = context['prompt'][:200] + "..." if len(context['prompt']) > 200 else context['prompt']
        f.write(f"**Prompt:**\n```\n{prompt_snippet}\n```\n\n")
        
        # Metrics table
        f.write("## Metrics\n\n")
        f.write("| Model | Latency (ms) | Tokens In | Tokens Out | Cost | Status |\n")
        f.write("|-------|--------------|-----------|------------|------|--------|\n")
        
        # Model 1 row
        status1 = "✅ OK" if result1["ok"] else f"❌ {result1.get('error', 'Error')}"
        tokens_in1 = str(result1["tokens_in"]) if result1["tokens_in"] is not None else "N/A"
        tokens_out1 = str(result1["tokens_out"]) if result1["tokens_out"] is not None else "N/A"
        cost1 = result1["cost"] if result1["cost"] is not None else "N/A"
        
        f.write(f"| {result1['model']} | {result1['latency_ms']} | {tokens_in1} | {tokens_out1} | {cost1} | {status1} |\n")
        
        # Model 2 row
        status2 = "✅ OK" if result2["ok"] else f"❌ {result2.get('error', 'Error')}"
        tokens_in2 = str(result2["tokens_in"]) if result2["tokens_in"] is not None else "N/A"
        tokens_out2 = str(result2["tokens_out"]) if result2["tokens_out"] is not None else "N/A"
        cost2 = result2["cost"] if result2["cost"] is not None else "N/A"
        
        f.write(f"| {result2['model']} | {result2['latency_ms']} | {tokens_in2} | {tokens_out2} | {cost2} | {status2} |\n")
        f.write("\n")
        
        # Outputs
        f.write("## Outputs\n\n")
        
        f.write(f"### {result1['model']}\n\n")
        f.write("```\n")
        f.write(result1["output"])
        f.write("\n```\n\n")
        
        f.write(f"### {result2['model']}\n\n")
        f.write("```\n")
        f.write(result2["output"])
        f.write("\n```\n\n")
        
        # Footer
        f.write("---\n")
        f.write("*Generated by any-llm Bench*\n")


def write_json(path: str, context: Dict[str, Any]) -> None:
    """Write a JSON report to the specified path."""
    
    # Create runs directory if it doesn't exist
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    with open(path, 'w') as f:
        json.dump(context, f, indent=2)


def generate_report_filename(timestamp: str) -> str:
    """Generate a filename for the report based on timestamp."""
    # Convert timestamp to filesystem-safe format
    safe_timestamp = timestamp.replace(":", "-").replace(" ", "_")
    return f"run-{safe_timestamp}"


def export_report(context: Dict[str, Any], base_dir: str = "runs") -> Dict[str, str]:
    """Export both Markdown and JSON reports and return the file paths."""
    
    filename = generate_report_filename(context["timestamp"])
    md_path = os.path.join(base_dir, f"{filename}.md")
    json_path = os.path.join(base_dir, f"{filename}.json")
    
    write_markdown(md_path, context)
    write_json(json_path, context)
    
    return {
        "markdown": md_path,
        "json": json_path
    }
